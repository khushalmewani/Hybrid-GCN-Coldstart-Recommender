{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MovieLens Cold-Start Recommendation System\n",
    "## Hybrid GCN with BERT Embeddings and Self-Supervised Learning\n",
    "\n",
    "This notebook implements a Graph Convolutional Network (GCN) for movie recommendations that handles cold-start items using semantic embeddings and contrastive learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Block 1: Setup & Data Download\n",
    "Download the MovieLens-100K dataset and extract it to the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "def download_movielens():\n",
    "    url = \"https://files.grouplens.org/datasets/movielens/ml-100k.zip\"\n",
    "    if not os.path.exists(\"./ml-100k\"):\n",
    "        print(\"Downloading MovieLens-100K dataset...\")\n",
    "        r = requests.get(url)\n",
    "        z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "        z.extractall(\"./\")\n",
    "        print(\"Done.\")\n",
    "    else:\n",
    "        print(\"Dataset already exists.\")\n",
    "\n",
    "download_movielens()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Block 2: Install Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch-geometric sentence-transformers pandas scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Block 3: Import Libraries\n",
    "Import all necessary libraries for PyTorch, PyTorch Geometric, BERT embeddings, and data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.utils import degree, dropout_adj\n",
    "from torch_geometric.data import Data\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Block 4: Configuration\n",
    "Define all hyperparameters and configuration settings for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    DATA_PATH = './ml-100k'\n",
    "    SPLIT_RATIO = 0.8        # 80% Warm / 20% Cold split\n",
    "    BERT_DIM = 384           # Output of all-MiniLM-L6-v2\n",
    "    EMBED_DIM = 64           # Latent space dimension\n",
    "    N_LAYERS = 3             # Number of GCN layers\n",
    "    DROPOUT = 0.1            \n",
    "    LR = 1e-3                \n",
    "    WEIGHT_DECAY = 1e-4      \n",
    "    EPOCHS = 50\n",
    "    BATCH_SIZE = 2048        \n",
    "    SSL_REG = 0.1            # Weight for Contrastive Loss\n",
    "    SSL_TEMP = 0.2           # Temperature for InfoNCE\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Block 5: Dataset Class - Cold Start Split\n",
    "Load MovieLens data, perform timestamp-based train/test split to simulate cold-start scenario, and generate BERT embeddings for item content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieLensColdStartDataset:\n",
    "    def __init__(self, root_path, split_ratio=0.8):\n",
    "        self.root = root_path\n",
    "        self.split_ratio = split_ratio\n",
    "        \n",
    "        # Load Interactions\n",
    "        self.interactions = pd.read_csv(\n",
    "            f\"{root_path}/u.data\", \n",
    "            sep='\\t', \n",
    "            names=['user_id', 'item_id', 'rating', 'timestamp']\n",
    "        )\n",
    "        \n",
    "        # Load Item Metadata (Title/Genre for BERT)\n",
    "        self.items = pd.read_csv(\n",
    "            f\"{root_path}/u.item\", \n",
    "            sep='|', \n",
    "            encoding='latin-1',\n",
    "            header=None,\n",
    "            usecols=[0, 1], \n",
    "            names=['item_id', 'title']\n",
    "        )\n",
    "        \n",
    "        self._process_ids()\n",
    "        self._time_split()\n",
    "        self._generate_bert_embeddings()\n",
    "        \n",
    "    def _process_ids(self):\n",
    "        self.user_enc = LabelEncoder()\n",
    "        self.item_enc = LabelEncoder()\n",
    "        \n",
    "        self.interactions['user_idx'] = self.user_enc.fit_transform(self.interactions['user_id'])\n",
    "        self.interactions['item_idx'] = self.item_enc.fit_transform(self.interactions['item_id'])\n",
    "        \n",
    "        self.num_users = len(self.user_enc.classes_)\n",
    "        self.num_items = len(self.item_enc.classes_)\n",
    "\n",
    "    def _time_split(self):\n",
    "        # Sort items by their first appearance timestamp\n",
    "        item_start_time = self.interactions.groupby('item_idx')['timestamp'].min().reset_index()\n",
    "        item_start_time = item_start_time.sort_values('timestamp')\n",
    "        \n",
    "        split_idx = int(len(item_start_time) * self.split_ratio)\n",
    "        \n",
    "        # Define Warm vs Cold items\n",
    "        train_items = item_start_time.iloc[:split_idx]['item_idx'].values\n",
    "        self.test_items = item_start_time.iloc[split_idx:]['item_idx'].values\n",
    "        \n",
    "        # Masks\n",
    "        self.train_mask = self.interactions['item_idx'].isin(train_items)\n",
    "        \n",
    "        print(f\"Total Items: {self.num_items}\")\n",
    "        print(f\"Warm Items (Train Graph): {len(train_items)}\")\n",
    "        print(f\"Cold Items (Zero-Shot Test): {len(self.test_items)}\")\n",
    "\n",
    "    def _generate_bert_embeddings(self):\n",
    "        # Sort items by index to ensure alignment\n",
    "        sorted_items = self.items.sort_values('item_id')\n",
    "        \n",
    "        print(\"Encoding item text with BERT (this may take a moment)...\")\n",
    "        bert = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.bert_feats = bert.encode(\n",
    "            sorted_items['title'].tolist(), \n",
    "            convert_to_tensor=True, \n",
    "            device=device\n",
    "        )\n",
    "\n",
    "    def get_graph_data(self):\n",
    "        # Only TRAIN interactions build the graph\n",
    "        train_df = self.interactions[self.train_mask]\n",
    "        \n",
    "        u_tensor = torch.tensor(train_df['user_idx'].values)\n",
    "        i_tensor = torch.tensor(train_df['item_idx'].values) + self.num_users\n",
    "        \n",
    "        edge_index = torch.stack([\n",
    "            torch.cat([u_tensor, i_tensor]),\n",
    "            torch.cat([i_tensor, u_tensor])\n",
    "        ], dim=0)\n",
    "        \n",
    "        return edge_index.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Block 6: LightGCN Convolution Layer\n",
    "Implementation of the LightGCN message passing layer with symmetric normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightGCNConv(MessagePassing):\n",
    "    def __init__(self):\n",
    "        super().__init__(aggr='add')\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Compute Normalization\n",
    "        row, col = edge_index\n",
    "        deg = degree(col, x.size(0), dtype=x.dtype)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "        return self.propagate(edge_index, x=x, norm=norm)\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        return norm.view(-1, 1) * x_j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Block 7: Hybrid GCN Model\n",
    "Main model combining BERT semantic embeddings with graph collaborative filtering. Uses layer-wise propagation and mean pooling across layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridGCN(nn.Module):\n",
    "    def __init__(self, num_users, num_items, bert_dim, embedding_dim, n_layers, dropout):\n",
    "        super(HybridGCN, self).__init__()\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.dropout = dropout\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # Semantic Projection\n",
    "        self.bert_proj = nn.Sequential(\n",
    "            nn.Linear(bert_dim, embedding_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # User Embeddings (Random Init)\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        nn.init.normal_(self.user_embedding.weight, std=0.1)\n",
    "\n",
    "        # Graph Propagation\n",
    "        self.convs = nn.ModuleList([LightGCNConv() for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, edge_index, bert_features):\n",
    "        item_emb_initial = self.bert_proj(bert_features)\n",
    "        user_emb_initial = self.user_embedding.weight\n",
    "        \n",
    "        x = torch.cat([user_emb_initial, item_emb_initial], dim=0)\n",
    "        \n",
    "        embs = [x]\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            if self.training:\n",
    "                x = F.dropout(x, p=self.dropout) \n",
    "            embs.append(x)\n",
    "            \n",
    "        embs = torch.stack(embs, dim=1)\n",
    "        final_emb = torch.mean(embs, dim=1)\n",
    "        \n",
    "        users_emb, items_emb = torch.split(final_emb, [self.num_users, self.num_items])\n",
    "        return users_emb, items_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Block 8: Loss Functions\n",
    "BPR (Bayesian Personalized Ranking) loss for recommendation and InfoNCE contrastive loss for self-supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpr_loss(users, pos_items, neg_items):\n",
    "    pos_scores = torch.sum(users * pos_items, dim=1)\n",
    "    neg_scores = torch.sum(users * neg_items, dim=1)\n",
    "    loss = -torch.mean(F.logsigmoid(pos_scores - neg_scores))\n",
    "    return loss\n",
    "\n",
    "def info_nce_loss(view1, view2, temp):\n",
    "    view1 = F.normalize(view1, dim=1)\n",
    "    view2 = F.normalize(view2, dim=1)\n",
    "    pos_score = (view1 * view2).sum(dim=1)\n",
    "    pos_score = torch.exp(pos_score / temp)\n",
    "    ttl_score = torch.matmul(view1, view2.transpose(0, 1))\n",
    "    ttl_score = torch.exp(ttl_score / temp).sum(dim=1)\n",
    "    loss = -torch.log(pos_score / ttl_score).mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Block 9: Training Loop\n",
    "Single epoch training with BPR loss and self-supervised graph learning (SGL) using edge dropout augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataset, optimizer):\n",
    "    model.train()\n",
    "    edge_index = dataset.get_graph_data()\n",
    "    bert_feats = dataset.bert_feats\n",
    "    \n",
    "    # 1. Forward\n",
    "    users_emb, items_emb = model(edge_index, bert_feats)\n",
    "    \n",
    "    # 2. BPR Loss (Random sampling for demo)\n",
    "    batch_users = torch.randint(0, dataset.num_users, (cfg.BATCH_SIZE,)).to(device)\n",
    "    # Note: In production, ensure positive items actually exist for the user\n",
    "    batch_pos = torch.randint(0, dataset.num_items, (cfg.BATCH_SIZE,)).to(device) \n",
    "    batch_neg = torch.randint(0, dataset.num_items, (cfg.BATCH_SIZE,)).to(device)\n",
    "    \n",
    "    loss_bpr = bpr_loss(users_emb[batch_users], items_emb[batch_pos], items_emb[batch_neg])\n",
    "    \n",
    "    # 3. SGL Loss\n",
    "    edge_index_1, _ = dropout_adj(edge_index, p=0.1)\n",
    "    edge_index_2, _ = dropout_adj(edge_index, p=0.1)\n",
    "    \n",
    "    user_view_1, _ = model(edge_index_1, bert_feats)\n",
    "    user_view_2, _ = model(edge_index_2, bert_feats)\n",
    "    \n",
    "    loss_sgl = info_nce_loss(user_view_1[batch_users], user_view_2[batch_users], cfg.SSL_TEMP)\n",
    "    \n",
    "    total_loss = loss_bpr + cfg.SSL_REG * loss_sgl\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return total_loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Block 10: Initialize Dataset\n",
    "Load and preprocess the MovieLens dataset with cold-start split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing Dataset...\")\n",
    "dataset = MovieLensColdStartDataset(cfg.DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Block 11: Initialize Model and Optimizer\n",
    "Create the Hybrid GCN model and Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing Model...\")\n",
    "model = HybridGCN(\n",
    "    num_users=dataset.num_users,\n",
    "    num_items=dataset.num_items,\n",
    "    bert_dim=cfg.BERT_DIM,\n",
    "    embedding_dim=cfg.EMBED_DIM,\n",
    "    n_layers=cfg.N_LAYERS,\n",
    "    dropout=cfg.DROPOUT\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=cfg.LR, weight_decay=cfg.WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Block 12: Training Execution\n",
    "Run the training loop for the specified number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting Training...\")\n",
    "for epoch in range(1, cfg.EPOCHS + 1):\n",
    "    loss = train_epoch(model, dataset, optimizer)\n",
    "    if epoch % 5 == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch}/{cfg.EPOCHS} | Loss: {loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
